<!DOCTYPE html>
<html>

<head>
  <meta http-equiv="Content-Type" content="text/html" charset="UTF-8" >
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"/>
  <title>Archives: 2017/9 | 刘昭 | Data Science</title>
  <meta name="description" content="" />
  <meta name="HandheldFriendly" content="True" />
  <meta name="MobileOptimized" content="320" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />

  <link rel="stylesheet" type="text/css" href="/css/screen.css" />
  <link rel="stylesheet" type="text/css" href="//fonts.googleapis.com/css?family=Noto+Serif:400,700,400italic|Open+Sans:700,400" />

  <meta name="generator" content="刘昭 | Data Science">

  
  
  

  
</head>


<body class="home-template">

  <header class="site-head"  style="background-image: url(//blog.ghost.org/content/images/2013/Nov/cover.png)" >
    <div class="vertical">
        <div class="site-head-content inner">
             <a class="blog-logo" href="/"><img src="http://omounq26l.bkt.clouddn.com/logo.jpg" alt="Blog Logo"/></a> 
            <h1 class="blog-title">刘昭 | Data Science</h1>
            <h2 class="blog-description"></h2>
        </div>
    </div>
</header>
  
<main class="content" role="main">
  
  <article class="post">
    <header class="post-header">
      <span class="post-meta">
      <time datetime="2017-09-10T14:22:10.000Z" itemprop="datePublished">
          2017-09-10
      </time>
    
    
    | 
    <a href='/tags/Machine-Learning/'>Machine Learning</a>
    
    
</span>
      <h2 class="post-title"><a href="/2017/09/10/牛顿法和梯度下降的区别/">牛顿法和梯度下降的区别</a></h2>
    </header>
    <section class="post-excerpt">
      <p>
      
        Part 1 牛顿法求解代价函数最小值牛顿法求解代价函数最小值，其最大的优点是不用像梯度函数那样不断迭代找到代价函数最小值，牛顿法牛逼的一点是经过一步迭代就可以求出满足代价函数最小的theta值。
接下来，我们看看牛顿法是如何做到的。
首先，我们的代价函数表达式如下：

牛顿法的迭代公式如下：






      
      </p>
      
      <p>
          <a href="/2017/09/10/牛顿法和梯度下降的区别/" class="excerpt-link">Read More...</a>
      </p>
      
    </section>
  </article>
  
  <article class="post">
    <header class="post-header">
      <span class="post-meta">
      <time datetime="2017-09-10T01:43:55.000Z" itemprop="datePublished">
          2017-09-10
      </time>
    
    
    | 
    <a href='/tags/Machine-Learning/'>Machine Learning</a>
    
    
</span>
      <h2 class="post-title"><a href="/2017/09/10/支持向量机的个人见解/">支持向量机的个人见解</a></h2>
    </header>
    <section class="post-excerpt">
      <p>
      
        Part 1 了解SVMSupport Vector Machine通俗讲是一个二分类模型，在特征空间寻找距离两类数据间隔最大的超平面，经过拉格朗日对偶问题的转换，最终成为求二次凸函数的的最优解。

如上图，这个间隔最大的超平面可以用来表示，SVM思想就是如何找到这个平面，问题用数学表达式表示就是
Part 2 深入SVM（线性可分）从Part 1最后得来的结果我们可以进行转化：因为现在的目标函数是二次的，约束条件是线性的，所以它是一个
      
      </p>
      
      <p>
          <a href="/2017/09/10/支持向量机的个人见解/" class="excerpt-link">Read More...</a>
      </p>
      
    </section>
  </article>
  
  <nav class="pagination" role="pagination">
    
    <span class="page-number">Page 1 of 1</span>
    
  </nav>
</main>


  
<footer class="site-footer">
  
  <div class="inner">
     <section class="copyright">All content copyright <a href="/">Zhao Liu</a> &copy; 2014 &bull; All rights reserved.</section>
     
  </div>
</footer>

  <script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script type="text/javascript" src="/js/jquery.fitvids.js"></script>
<script type="text/javascript" src="/js/index.js"></script>






</body>
</html>
